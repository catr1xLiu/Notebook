
---

> [!quote] Definition
> ## Matrix Product

Let $A \in M_{m \times n}(\mathbb{R})$ and $B \in M_{n \times p}(\mathbb{R})$ with $B = [\vec{b}_1\ \vec{b}_2\ \cdots\ \vec{b}_p]$.

We define the <u><strong style="color:#dab1da">product</strong></u> $AB$ as:

$$
AB = A[\vec{b}_1\ \vec{b}_2\ \cdots\ \vec{b}_p] = [A\vec{b}_1\ A\vec{b}_2\ \cdots\ A\vec{b}_p]
$$

**Note:** Since $A$ is $m \times n$ and each $\vec{b}_i$ is $n \times 1$, each product $A\vec{b}_i$ is $m \times 1$. Since there are $p$ columns in $B$, the resulting product $AB$ is $m \times p$.

**Important:** The product $AB$ is only defined when the number of columns in $A$ equals the number of rows in $B$.

> [!example] Example
> ## Computing Matrix Products

Let $A = \begin{bmatrix} 2 & 1 \\ -1 & 3 \end{bmatrix}$, $B = \begin{bmatrix} 1 & 2 & 0 \\ -1 & 5 & 7 \end{bmatrix}$, and $C = \begin{bmatrix} 2 & -2 \\ 3 & 1 \end{bmatrix}$.

Compute: $AC$, $CA$, $AB$, $BA$

> [!success]- Solution (Click to expand)
> 
> **a) Computing $AC$:**
> 
> $$
> AC = \begin{bmatrix} 2 & 1 \\ -1 & 3 \end{bmatrix} \begin{bmatrix} 2 & -2 \\ 3 & 1 \end{bmatrix}
> $$
> 
> Computing column by column:
> - First column: $A\begin{bmatrix} 2 \\ 3 \end{bmatrix} = \begin{bmatrix} 2(2) + 1(3) \\ -1(2) + 3(3) \end{bmatrix} = \begin{bmatrix} 7 \\ 7 \end{bmatrix}$
> - Second column: $A\begin{bmatrix} -2 \\ 1 \end{bmatrix} = \begin{bmatrix} 2(-2) + 1(1) \\ -1(-2) + 3(1) \end{bmatrix} = \begin{bmatrix} -3 \\ 5 \end{bmatrix}$
> 
> Therefore: $AC = \begin{bmatrix} 7 & -3 \\ 7 & 5 \end{bmatrix}$
> 
> **b) Computing $CA$:**
> 
> $$
> CA = \begin{bmatrix} 2 & -2 \\ 3 & 1 \end{bmatrix} \begin{bmatrix} 2 & 1 \\ -1 & 3 \end{bmatrix}
> $$
> 
> Computing column by column:
> - First column: $C\begin{bmatrix} 2 \\ -1 \end{bmatrix} = \begin{bmatrix} 2(2) + (-2)(-1) \\ 3(2) + 1(-1) \end{bmatrix} = \begin{bmatrix} 6 \\ 5 \end{bmatrix}$
> - Second column: $C\begin{bmatrix} 1 \\ 3 \end{bmatrix} = \begin{bmatrix} 2(1) + (-2)(3) \\ 3(1) + 1(3) \end{bmatrix} = \begin{bmatrix} -4 \\ 6 \end{bmatrix}$
> 
> Therefore: $CA = \begin{bmatrix} 6 & -4 \\ 5 & 6 \end{bmatrix}$
> 
> **Notice:** $AC \neq CA$, demonstrating that matrix multiplication is **not commutative**.
> 
> **c) Computing $AB$:**
> 
> $$
> AB = \begin{bmatrix} 2 & 1 \\ -1 & 3 \end{bmatrix} \begin{bmatrix} 1 & 2 & 0 \\ -1 & 5 & 7 \end{bmatrix}
> $$
> 
> Computing each column:
> - First column: $\begin{bmatrix} 2(1) + 1(-1) \\ -1(1) + 3(-1) \end{bmatrix} = \begin{bmatrix} 1 \\ -4 \end{bmatrix}$
> - Second column: $\begin{bmatrix} 2(2) + 1(5) \\ -1(2) + 3(5) \end{bmatrix} = \begin{bmatrix} 9 \\ 13 \end{bmatrix}$
> - Third column: $\begin{bmatrix} 2(0) + 1(7) \\ -1(0) + 3(7) \end{bmatrix} = \begin{bmatrix} 7 \\ 21 \end{bmatrix}$
> 
> Therefore: $AB = \begin{bmatrix} 1 & 9 & 7 \\ -4 & 13 & 21 \end{bmatrix}$
> 
> **d) Computing $BA$:**
> 
> The product $BA$ requires $B$ to be $n \times p$ and $A$ to be $p \times k$. Here $B$ is $2 \times 3$ and $A$ is $2 \times 2$. Since the number of columns in $B$ (which is 3) does not equal the number of rows in $A$ (which is 2), the product $BA$ is **undefined**.

> [!info] Info
> ## Entry-wise Computation

While the column-by-column definition is fundamental, we can also compute individual entries of the product.

**The $(i,j)$-entry of $AB$:** If $A$ is $m \times n$ and $B$ is $n \times p$, then the $(i,j)$-entry of $AB$ is obtained by taking the <u><strong style="color:#dab1da">dot product</strong></u> of the $i^{\text{th}}$ row of $A$ with the $j^{\text{th}}$ column of $B$.

If $\vec{r}_i$ denotes the $i^{\text{th}}$ row of $A$ and $\vec{c}_j$ denotes the $j^{\text{th}}$ column of $B$, then:

$$
(AB)_{ij} = \vec{r}_i \cdot \vec{c}_j
$$

> [!fact] Theorem
> ## Properties of Matrix Multiplication

Let $A$, $B$, $C$ be appropriately sized matrices and $k \in \mathbb{R}$.

- <u><strong style="color:#dab1da">Associativity</strong></u>: $A(BC) = (AB)C$
- <u><strong style="color:#dab1da">Left Distributivity</strong></u>: $A(B + C) = AB + AC$
- <u><strong style="color:#dab1da">Right Distributivity</strong></u>: $(A + B)C = AC + BC$
- <u><strong style="color:#dab1da">Scalar Multiplication Compatibility</strong></u>: $k(AB) = (kA)B = A(kB)$
- <u><strong style="color:#dab1da">Transpose of a Product</strong></u>: $(AB)^T = B^T A^T$

> [!info]- Why $(AB)^T = B^T A^T$ (Click to expand)
> 
> The transpose property may seem counterintuitive. Let's understand why it works.
> 
> By definition of transpose, the $i^{\text{th}}$ row of $(AB)^T$ is the $i^{\text{th}}$ column of $AB$.
> 
> Let $\vec{r}_j$ denote the $j^{\text{th}}$ row of $A$. Then the $i^{\text{th}}$ column of $AB$ is computed as:
> 
> $$
> \begin{bmatrix}
> \vec{r}_1 \cdot \vec{b}_i \\
> \vec{r}_2 \cdot \vec{b}_i \\
> \vdots \\
> \vec{r}_m \cdot \vec{b}_i
> \end{bmatrix}
> $$
> 
> where $\vec{b}_i$ is the $i^{\text{th}}$ column of $B$.
> 
> Since $\vec{u} \cdot \vec{v} = \vec{v} \cdot \vec{u}$, we have $\vec{r}_j \cdot \vec{b}_i = \vec{b}_i \cdot \vec{r}_j$.
> 
> Now, $\vec{r}_j$ as a row vector becomes the $j^{\text{th}}$ column of $A^T$, and $\vec{b}_i$ becomes the $i^{\text{th}}$ row of $B^T$.
> 
> Therefore, the $i^{\text{th}}$ row of $(AB)^T$ equals the $i^{\text{th}}$ row of $B^T$ multiplied by $A^T$, which is exactly the $i^{\text{th}}$ row of $B^T A^T$.

> [!hint] Hint
> ## Non-Commutativity of Matrix Multiplication

**Important:** In general, $AB \neq BA$. Matrix multiplication is **not commutative**.

Even when both $AB$ and $BA$ are defined (which requires $A$ to be $m \times n$ and $B$ to be $n \times m$), the two products are typically different.

When $AB = BA$ for two matrices, we say that $A$ and $B$ <u><strong style="color:#dab1da">commute</strong></u>.

> [!example] Example
> ## Verifying That Matrices Commute

Show that $A = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$ and $B = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}$ commute.

> [!success]- Solution (Click to expand)
> 
> **Computing $AB$:**
> 
> $$
> AB = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix} = \begin{bmatrix} 2 & 3 \\ 0 & 2 \end{bmatrix}
> $$
> 
> **Computing $BA$:**
> 
> $$
> BA = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 2 & 3 \\ 0 & 2 \end{bmatrix}
> $$
> 
> Since $AB = BA = \begin{bmatrix} 2 & 3 \\ 0 & 2 \end{bmatrix}$, the matrices $A$ and $B$ commute.

> [!quote] Definition
> ## The Identity Matrix

There is one very important matrix that usually commutes with all square matrices of the same size.

The <u><strong style="color:#dab1da">identity matrix</strong></u> $I_n$ is the $n \times n$ matrix such that:

$$
(I_n)_{ij} = \begin{cases}
1 & \text{when } i = j \\
0 & \text{when } i \neq j
\end{cases}
$$

In other words:

$$
I_n = \begin{bmatrix}
1 & 0 & 0 & \cdots & 0 \\
0 & 1 & 0 & \cdots & 0 \\
0 & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 1
\end{bmatrix}
$$

When the size of the identity matrix is clear from context, we often just write $I$.

**For example:**

$$
I_3 = \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
$$

> [!fact] Theorem
> ## Properties of the Identity Matrix

Let $A \in M_{m \times n}(\mathbb{R})$ and $I_n = I$ be the $n \times n$ identity matrix. Then:

$$
AI_n = I_m A = A
$$

**Note:** If $A$ is $m \times n$, we need to use $I_m$ on the left and $I_n$ on the right, since the dimensions must match for multiplication.

> [!success]- Proof (Click to expand)
> 
> Let $\vec{e}_i$ represent the vector that is all zeros except the $i^{\text{th}}$ entry which is $1$. Then:
> 
> $$
> I_n = \begin{bmatrix} \vec{e}_1 & \vec{e}_2 & \cdots & \vec{e}_n \end{bmatrix}
> $$
> 
> We have:
> 
> $$
> AI_n = A\begin{bmatrix} \vec{e}_1 & \vec{e}_2 & \cdots & \vec{e}_n \end{bmatrix} = \begin{bmatrix} A\vec{e}_1 & A\vec{e}_2 & \cdots & A\vec{e}_n \end{bmatrix}
> $$
> 
> Now, for any matrix $A$, we know that $A\vec{e}_i$ extracts the $i^{\text{th}}$ column of $A$ (this follows from the definition of the matrix-vector product).
> 
> Therefore:
> 
> $$
> AI_n = \begin{bmatrix} \text{column}_1(A) & \text{column}_2(A) & \cdots & \text{column}_n(A) \end{bmatrix} = A
> $$
> 
> The proof that $I_m A = A$ follows similarly by considering row operations.

> [!quote] Definition
> ## Matrix Powers and Diagonal Matrices

**Matrix Powers:** If $A$ is a square matrix (i.e., $A \in M_{n \times n}(\mathbb{R})$), we define:

$$
A^k = \underbrace{A \cdot A \cdot A \cdots A}_{k \text{ times}}
$$

for any positive integer $k \geq 1$, and $A^1 = A$.

**Diagonal Matrices:** A matrix $A$ is called <u><strong style="color:#dab1da">diagonal</strong></u> whenever $a_{ij} = 0$ for all $i \neq j$.

For example, $I_n$ and $\begin{bmatrix} 2 & 0 & 0 & 0 \\ 0 & -1 & 0 & 0 \\ 0 & 0 & 9 & 0 \\ 0 & 0 & 0 & 3 \end{bmatrix}$ are both diagonal matrices.

> [!example] Example
> ## Powers of a Diagonal Matrix

Let $D = \begin{bmatrix} -2 & 0 \\ 0 & 5 \end{bmatrix}$. Compute $D^2$ and $D^6$.

> [!success]- Solution (Click to expand)
> 
> **Computing $D^2$:**
> 
> $$
> D^2 = \begin{bmatrix} -2 & 0 \\ 0 & 5 \end{bmatrix} \begin{bmatrix} -2 & 0 \\ 0 & 5 \end{bmatrix} = \begin{bmatrix} 4 & 0 \\ 0 & 25 \end{bmatrix}
> $$
> 
> **Computing $D^6$:**
> 
> We can observe the pattern: for a diagonal matrix, raising it to a power simply raises each diagonal entry to that power:
> 
> $$
> D^6 = \begin{bmatrix} (-2)^6 & 0 \\ 0 & 5^6 \end{bmatrix} = \begin{bmatrix} 64 & 0 \\ 0 & 15625 \end{bmatrix}
> $$
> 
> **General Pattern:** For any diagonal matrix $D = \begin{bmatrix} d_1 & 0 & \cdots & 0 \\ 0 & d_2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & d_n \end{bmatrix}$, we have:
> 
> $$
> D^k = \begin{bmatrix} d_1^k & 0 & \cdots & 0 \\ 0 & d_2^k & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & d_n^k \end{bmatrix}
> $$

