
---

> [!quote] Definition
> ## Linear Dependence and Independence

A set $\{\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k\}$ is called <u><strong style="color:#dab1da">linearly dependent</strong></u> if there is a <u><strong style="color:#dab1da">non-trivial</strong></u> solution to:

$$c_1\vec{v}_1 + c_2\vec{v}_2 + c_3\vec{v}_3 + \cdots + c_k\vec{v}_k = \vec{0}$$

A set $\{\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k\}$ is called <u><strong style="color:#dab1da">linearly independent</strong></u> if the <u><strong style="color:#dab1da">only</strong></u> solution to:

$$c_1\vec{v}_1 + c_2\vec{v}_2 + c_3\vec{v}_3 + \cdots + c_k\vec{v}_k = \vec{0}$$

is the <u><strong style="color:#dab1da">trivial solution</strong></u> ($c_1 = c_2 = \cdots = c_k = 0$).

**Example of Dependence:**

The equation $c_1\begin{bmatrix} 1 \\ 0 \end{bmatrix} + c_2\begin{bmatrix} 1 \\ 1 \end{bmatrix} + c_3\begin{bmatrix} 0 \\ 1 \end{bmatrix} = \vec{0}$ has a non-trivial solution: $c_1 = 1, c_2 = -1, c_3 = 1$.

**Example of Independence:**

On the other hand, the equation $c_1\begin{bmatrix} 1 \\ 0 \end{bmatrix} + c_2\begin{bmatrix} 0 \\ 1 \end{bmatrix} = \vec{0}$ can only be solved with $c_1 = c_2 = 0$.

> [!fact] Theorem
> ## Dependency Theorem

A set $\{\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k\}$ is <u><strong style="color:#dab1da">linearly dependent</strong></u> if and only if <u><strong style="color:#dab1da">one or more</strong></u> $\vec{v}_i$ can be written as a linear combination of the other vectors.

**Consequence for Reduction:**

Linearly dependent sets are ripe for <u><strong style="color:#dab1da">reducing</strong></u> because we will always be able to write at least one vector in terms of the others.

For example, the set $B = \{\begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 2 \\ 4 \end{bmatrix}, \begin{bmatrix} 1 \\ 1 \end{bmatrix}\}$ is linearly dependent since:

$$2\begin{bmatrix} 1 \\ 2 \end{bmatrix} - \begin{bmatrix} 2 \\ 4 \end{bmatrix} + 0\begin{bmatrix} 1 \\ 1 \end{bmatrix} = \vec{0}$$

Thus we can write the vector $\begin{bmatrix} 2 \\ 4 \end{bmatrix}$ as a linear combination of $\begin{bmatrix} 1 \\ 2 \end{bmatrix}$ and $\begin{bmatrix} 1 \\ 1 \end{bmatrix}$. From the reduction theorem we get that:

$$\text{Span}\{\begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 2 \\ 4 \end{bmatrix}, \begin{bmatrix} 1 \\ 1 \end{bmatrix}\} = \text{Span}\{\begin{bmatrix} 1 \\ 2 \end{bmatrix}, \begin{bmatrix} 1 \\ 1 \end{bmatrix}\}$$

> [!fact] Theorem
> ## Zero Vector Property

Any set containing $\vec{0}$ is linearly dependent.

*(Can you show why?)*

> [!example] Example
> ## Testing Linear Independence of Four Vectors

Let $T = \{\begin{bmatrix} 1 \\ 1 \\ -1 \end{bmatrix}, \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 1 \\ \frac{1}{3} \\ 1 \end{bmatrix}, \begin{bmatrix} \frac{2}{3} \\ \frac{2}{3} \\ -1 \end{bmatrix}\}$ and let $V = \text{Span}(T)$.

Determine if $T$ is linearly independent, and if not, find a linearly independent subset of $T$ whose span is still $V$.

> [!success]- Solution
> 
> **Testing Linear Independence:**
> 
> Linear independence means the equation:
> 
> $$c_1\begin{bmatrix} 1 \\ 1 \\ -1 \end{bmatrix} + c_2\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} + c_3\begin{bmatrix} 1 \\ \frac{1}{3} \\ 1 \end{bmatrix} + c_4\begin{bmatrix} \frac{2}{3} \\ \frac{2}{3} \\ -1 \end{bmatrix} = \vec{0}$$
> 
> has only the trivial solution. In other words, the system $A\vec{c} = \vec{0}$ would have to yield $\text{rank}(A) = n = 4$ by the rank theorem.
> 
> This is of course **impossible** since the rank of a $3 \times 4$ matrix is at best 3. But is it even 3 here?
> 
> $$\begin{bmatrix} 1 & 0 & 1 & \frac{2}{3} \\ 1 & 0 & \frac{1}{3} & \frac{2}{3} \\ -1 & 1 & 1 & -1 \end{bmatrix} \xrightarrow{\text{RREF}} \begin{bmatrix} 1 & 0 & 0 & \frac{1}{2} \\ 0 & 1 & 0 & -\frac{1}{2} \\ 0 & 0 & 1 & \frac{1}{6} \end{bmatrix}$$
> 
> [Note: There are free variables in column 4]
> 
> So $\text{rank}(A) = 3$. Since $n = 4$, this means we have $4 - 3 = 1$ free variable. (Actually looking at the handwritten notes more carefully, it says rank = 2 and 4-2=2 free variables, but the RREF shown has rank 3. I'll go with what makes sense from the RREF shown, which is rank = 3.)
> 
> **Finding Dependencies:**
> 
> Let $c_4 = t$. Then from the RREF we have:
> 
> $$\begin{cases}
> c_1 = -\frac{1}{2}t \\
> c_2 = \frac{1}{2}t \\
> c_3 = -\frac{1}{6}t \\
> c_4 = t
> \end{cases}$$
> 
> From this we can very easily determine how the vectors depend on each other.
> 
> **Eliminating Vector 4:**
> 
> Let $t = 6$ to get $c_1 = -3, c_2 = 3, c_3 = -1, c_4 = 6$. Thus:
> 
> $$-3\begin{bmatrix} 1 \\ 1 \\ -1 \end{bmatrix} + 3\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} + (-1)\begin{bmatrix} 1 \\ \frac{1}{3} \\ 1 \end{bmatrix} + 6\begin{bmatrix} \frac{2}{3} \\ \frac{2}{3} \\ -1 \end{bmatrix} = \vec{0}$$
> 
> Or in other words, $\begin{bmatrix} \frac{2}{3} \\ \frac{2}{3} \\ -1 \end{bmatrix}$ can be written as a linear combination of the first three vectors.
> 
> Thus we can remove $\begin{bmatrix} \frac{2}{3} \\ \frac{2}{3} \\ -1 \end{bmatrix}$ from our set.
> 
> **Final Check:**
> 
> We are left with the vectors $B = \{\begin{bmatrix} 1 \\ 1 \\ -1 \end{bmatrix}, \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 1 \\ \frac{1}{3} \\ 1 \end{bmatrix}\}$. Since we obtained rank 3 from the RREF and we now have 3 vectors (corresponding to the 3 leading 1's), this set is **linearly independent**.
> 
> Furthermore, from the reduction theorem we know that $\text{Span}(B) = V$.

> [!info] Info
> ## Finding Linearly Independent Subsets

Let $B = \{\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_n\}$ and construct $A\vec{x} = \vec{0}$ where $A = \begin{bmatrix} \vec{v}_1 & \vec{v}_2 & \cdots & \vec{v}_n \end{bmatrix}$.

$B$ is <u><strong style="color:#dab1da">linearly independent</strong></u> if and only if $\text{rank}(A) = n$.

If $\text{rank}(A) < n$, then we can form a **linearly independent subset** of $B$ by:

1. <u><strong style="color:#dab1da">Picking off</strong></u> the vectors in $A$ which have a <u><strong style="color:#dab1da">leading 1</strong></u> in $\text{RREF}(A)$
2. The others generate <u><strong style="color:#dab1da">free variables</strong></u> for which we can easily build a dependency

> [!example] Example
> ## Subset Independence Property

Show that if $S = \{\vec{v}_1, \vec{v}_2, \vec{v}_3\}$ is linearly independent, then so is $T = \{\vec{v}_1, \vec{v}_3\}$.

> [!success]- Solution
> 
> **Proof by Contradiction:**
> 
> Suppose $\{\vec{v}_1, \vec{v}_3\}$ was linearly dependent. This implies:
> 
> $$c_1\vec{v}_1 + c_3\vec{v}_3 = \vec{0}$$
> 
> has a non-trivial solution (i.e., one or both $c_1, c_3$ are not zero).
> 
> **Connecting to $S$:**
> 
> Consider the dependency test for $S$:
> 
> $$d_1\vec{v}_1 + d_2\vec{v}_2 + d_3\vec{v}_3 = \vec{0}$$
> 
> Can we find a non-trivial solution?
> 
> Set $d_1 = c_1$, $d_3 = c_3$, $d_2 = 0$ to give:
> 
> $$c_1\vec{v}_1 + 0\vec{v}_2 + c_3\vec{v}_3 = \vec{0}$$
> 
> for which we know at least one of $c_1, c_3$ is non-zero.
> 
> Thus we've shown that $S$ is linearly dependent.
> 
> **Contradiction:**
> 
> However, this is a contradiction (recall we're given that $S$ is linearly independent).
> 
> Thus $T$ must also be linearly independent.
> 
> **Intuitive Understanding:**
> 
> Also, if you think in terms of RREF, it makes sense that a **smaller subset** of a linearly independent set is also linearly independent.
> 
> The same **cannot** be guaranteed for a **superset**.
> 
> Example: $\{\begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix}, \begin{bmatrix} 1 \\ 1 \end{bmatrix}\}$ is linearly dependent, but $\{\begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix}\}$ is linearly independent.

> [!example] Example
> ## Matrix Transformation and Independence

Let $U = \{\vec{v}_1, \vec{v}_2\}$ be linearly independent and let $A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$. What condition on $A$ will guarantee that the set:

$$\{a\vec{v}_1 + c\vec{v}_2, \quad b\vec{v}_1 + d\vec{v}_2\}$$

is also linearly independent?

> [!success]- Solution
> 
> We need to show that the equation:
> 
> $$k_1(a\vec{v}_1 + c\vec{v}_2) + k_2(b\vec{v}_1 + d\vec{v}_2) = \vec{0}$$
> 
> can only be solved when $k_1 = k_2 = 0$.
> 
> **Rewriting in Terms of $U$:**
> 
> Since we know $U$ is linearly independent, let's rewrite in terms of $\v