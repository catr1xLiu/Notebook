
---

> [!abstract] Introduction
> ## Span and Linear Combinations

When working with vectors, we often want to describe all possible vectors that can be created through <u><strong style="color:#dab1da">linear combinations</strong></u>. The concept of **span** formalizes this idea, allowing us to understand which vectors can be "reached" by scaling and adding a given set of vectors.

> [!quote] Definition
> ## Span of a Set

Let $B = \{\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k\}$ be a set of vectors in $\mathbb{R}^n$.

We define **all possible linear combinations** of $B$ as the <u><strong style="color:#dab1da">span</strong></u> of $\mathbb{R}$ and write:

$$\text{Span}(B) = \{c_1\vec{v}_1 + c_2\vec{v}_2 + \cdots + c_k\vec{v}_k\} \quad (c_k \in \mathbb{R})$$

If we let $V = \text{Span}(B)$, then we say $B$ is a <u><strong style="color:#dab1da">spanning set</strong></u> for $V$, or that $V$ is <u><strong style="color:#dab1da">spanned by</strong></u> $B$.

> [!example] Example
> ## Describing the Span of Two Vectors

Let $\vec{v}_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$ and $\vec{v}_2 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$. Describe the set $\vec{x} = c_1\vec{v}_1 + c_2\vec{v}_2$ for all possible values $c_1$ and $c_2$.

> [!success]- Solution
> 
> **Graphical Analysis:**
> 
> [Insert graph: Two vectors in ℝ² with v₁ pointing diagonally up-right and v₂ pointing horizontally right]
> 
> - The equation $\vec{x} = c_1\vec{v}_1$ represents a **line** through the origin along $\vec{v}_1$
> - The equation $\vec{x} = c_2\vec{v}_2$ represents a **line** through the origin along $\vec{v}_2$
> 
> Since we can shrink/grow/reflect the vectors $\vec{v}_1$ and $\vec{v}_2$ however we want, it seems reasonable that we should be able to "arrive" at any point $\begin{bmatrix} a \\ b \end{bmatrix} \in \mathbb{R}^2$ by a suitable choice of $c_1$ and $c_2$.
> 
> **Algebraic Verification:**
> 
> Indeed, solving $c_1\vec{v}_1 + c_2\vec{v}_2 = \vec{x}$ yields:
> 
> $$c_1\begin{bmatrix} 1 \\ 1 \end{bmatrix} + c_2\begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \Rightarrow \begin{bmatrix} 1 & 1 \\ 1 & 0 \end{bmatrix}\begin{bmatrix} c_1 \\ c_2 \end{bmatrix} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}$$
> 
> Row reducing the augmented matrix:
> 
> $$\begin{bmatrix} 1 & 1 & x_1 \\ 1 & 0 & x_2 \end{bmatrix} \to \begin{bmatrix} 0 & 1 & x_1-x_2 \\ 1 & 0 & x_2 \end{bmatrix} \to \begin{bmatrix} 1 & 0 & x_2 \\ 0 & 1 & x_1 - x_2 \end{bmatrix}$$
> 
> This gives us $c_1 = x_2$ and $c_2 = x_1 - x_2$.
> 
> **Conclusion:**
> 
> The vectors $\vec{v}_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$ and $\vec{v}_2 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$ can "cover" all of $\mathbb{R}^2$. Therefore:
> 
> $$\text{Span}\{\vec{v}_1, \vec{v}_2\} = \mathbb{R}^2$$

> [!info] Info
> ## Determining if a Vector is in a Span

In general, given a set of vectors $B = \{\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k\}$ and a random vector $\vec{w}$, to determine if $\vec{w} \in \text{Span}(B)$, we need to solve the system:

$$A\vec{x} = \vec{w}$$

where $A = \begin{bmatrix} \vec{v}_1 & \vec{v}_2 & \cdots & \vec{v}_k \end{bmatrix}$

If this system is **consistent**, then $\vec{w} \in \text{Span}(B)$.

> [!example] Example
> ## Finding a Smaller Spanning Set

Consider the set $V = \text{Span}\{\begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 \\ 1 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix}\}$. Is there a smaller set $U$ such that $\text{Span}(U) = \text{Span}(V)$?

> [!success]- Solution
> 
> By definition, $\vec{x} \in \text{Span}(V)$ means:
> 
> $$\vec{x} = c_1\begin{bmatrix} 1 \\ 0 \end{bmatrix} + c_2\begin{bmatrix} 1 \\ 1 \end{bmatrix} + c_3\begin{bmatrix} 0 \\ 1 \end{bmatrix}$$
> 
> We know from the previous example that $\text{Span}(V) = \mathbb{R}^2$. However, we also note that the vectors $\begin{bmatrix} 1 \\ 0 \end{bmatrix}$ and $\begin{bmatrix} 1 \\ 1 \end{bmatrix}$ can more easily cover $\mathbb{R}^2$.
> 
> As such, letting $U = \{\begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix}\}$ will give us $\text{Span}(U) = \text{Span}(V)$.
> 
> **Note:** It is also correct to use $\{\begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 \\ 1 \end{bmatrix}\}$ or even $\{\begin{bmatrix} 1 \\ 1 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix}\}$.
> 
> **Key Insight:**
> 
> To cover $\mathbb{R}^2$, we really only need **2** vectors (not just any vectors though - e.g., $\{\begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 2 \\ 0 \end{bmatrix}\}$ wouldn't work).

> [!fact] Theorem
> ## Reduction Theorem

$$\text{Span}\{\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k\} = \text{Span}\{\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_{k-1}\}$$

if and only if $\vec{v}_k$ can be written as a linear combination of $\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_{k-1}$ (where $\vec{v}_i \in \mathbb{R}^n$).

*See proof in Section 4.2*

**Application to Previous Example:**

We previously argued that:

$$\text{Span}\{\begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 1 \\ 1 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix}\} = \text{Span}\{\begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix}\}$$

Or in other words, we can "get to" the zero vector $\vec{0}$ in a non-trivial fashion:

$$\begin{bmatrix} 1 \\ 0 \end{bmatrix} - \begin{bmatrix} 1 \\ 1 \end{bmatrix} + \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \vec{0}$$